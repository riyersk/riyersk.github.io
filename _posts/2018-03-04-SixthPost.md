---
layout: post
title: Project Fletcher Reflection
---

Last Friday, we presented our third individual project, which was an unsupervised learning project with NLP. I chose to do my project on Patents. I scraped download links from [here]("https://bulkdata.uspto.gov"), and parsed the resulting textfiles, storing the results in a database.  Then, I trained a doc2vec model on the abstracts of the patents, so that each patent abstract was represented by a 300 dimensional vector.  Then, the plan was to cluster the vectors by cosine similarity using DBSCAN, and then visualize the clusters through time using TSNE.  Essentially, I wanted to show how the frontiers of technology move over time.  Unfortunately, sklearn's DBSCAN is very memory intensive, so even when I used an EC2 instance with 1.9 TB of RAM, it was not able to run the algorithm.  I tried gamely to get around this, but to no avail. Instead, I just ran the DBSCAN on the patents from the year 2000 and showed a TSNE plot.

At some point, I would like to revisit this project and do what I originally wanted to do.  I've forked the scikit learn repository, and it's on my list to look into the DBSCAN process and see if I can make it less memory intensive (i.e., compromise on run time).  Then, I can run the process again and see if I could get that visualization I was looking for.  Additionally, I didn't get a chance to work with them, but I also generated document vectors for the background+summary of the patents.  I'd like to do the same process on those vectors to see if they can reveal insights not rendered obvious by the abstracts alone.  Third, the results of the DBSCAN (for 2000) suggested 204 topics.  Originally, I ruled out trying NMF,LSA, and LDA because you need to have a rough idea of topic number going in, and I suspected that my number of topics would be in the thousands.  I had no desire to try different values of thousands for topic modeling.  However, given that the DBSCAN suggested only 204 topics, for 2000 at least, perhaps it would be interesting to try NMF,LSA, and LDA with the number of topics around 200 to see what happens.

There are a lot of possible extensions with this project.  I stored the assignee of the patents.  It'd be interesting to color the vectors by assignee and see how the vector cloud of different companies' patents changes over time.  For each patent, I also stored a list of patent numbers that the patent includes as references.  There's probably a network theory exploration there somewhere.  Also, I only downloaded patents that were available via bulk download as a .txt file.  After 2001, the patent office switched to XML format.  It would be a cool extension to learn how to work with XML, add those additional patents to the database, and run all these same processes.  Also, near the end of the project, we studied recommender systems.  It'd be interesting to try and build a recommender system for patents.  This might be useful for prior art searches and for infringement suits.

My ultra pipe dream, of course, is to automate the job of the Patent Examiner and perhaps, even the job of a Patent Agent.  I don't think the technology is there yet, but we'll see.
